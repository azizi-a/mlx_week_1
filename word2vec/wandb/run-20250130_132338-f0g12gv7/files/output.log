Epoch 1, Loss: 2.1378:   5%|█▎                          | 940/20769 [01:11<25:03, 13.19it/s]
Traceback (most recent call last):
  File "/home/azizi/Code/MLX/week_1/word2vec/run_word2vec.py", line 38, in <module>
    model = train_model(corpus, words_to_ids, ids_to_words)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/azizi/Code/MLX/week_1/word2vec/SkipGram.py", line 58, in train_model
    optimizer.step()
  File "/home/azizi/miniconda3/envs/mlx_w1/lib/python3.12/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^
  File "/home/azizi/miniconda3/envs/mlx_w1/lib/python3.12/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/azizi/miniconda3/envs/mlx_w1/lib/python3.12/site-packages/torch/optim/adam.py", line 223, in step
    adam(
  File "/home/azizi/miniconda3/envs/mlx_w1/lib/python3.12/site-packages/torch/optim/optimizer.py", line 154, in maybe_fallback
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/azizi/miniconda3/envs/mlx_w1/lib/python3.12/site-packages/torch/optim/adam.py", line 784, in adam
    func(
  File "/home/azizi/miniconda3/envs/mlx_w1/lib/python3.12/site-packages/torch/optim/adam.py", line 378, in _single_tensor_adam
    exp_avg.lerp_(grad, 1 - beta1)
KeyboardInterrupt
